{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Data Engineering\n",
    "\n",
    "## Chapter 1: Introduction to Data Engineering\n",
    "\n",
    "### What is data engineering?\n",
    "\n",
    "#### In comes the data engineer\n",
    "* Data is scattered\n",
    "* Not optimized for analyses\n",
    "* Legacy code is causing corrupt data\n",
    "\n",
    "**Data Engineer** to the rescue!\n",
    "\n",
    "#### Data engineers: making your life easier\n",
    "* Gather data from different sources\n",
    "* Optimized database for analyses\n",
    "* Removed corrupt data\n",
    "\n",
    "**Data Scientist's** life just got easier!\n",
    "\n",
    "#### Definition of the job\n",
    "> An engineer that develops, constructs, tests, and maintains architectures such as databases and large-scale processing systems.\n",
    "\n",
    "* Processing large amounts of data\n",
    "* Use of clusters of machines\n",
    "\n",
    "#### Data Enginner vs. Data Scientist\n",
    "Data Engineer:\n",
    "* Develop scalable data architecture\n",
    "* Streamline data acquisition\n",
    "* Set up processes to bring together data\n",
    "* Clean corrupt data\n",
    "* Well versed in cloud technology\n",
    "\n",
    "Data Scientist:\n",
    "* Mining data for patterns\n",
    "* Statistical modeling\n",
    "* Predictive models using machine learning\n",
    "* Monitor business processes\n",
    "* Clean outliers in data\n",
    "\n",
    "### Tools of the data engineer\n",
    "\n",
    "#### Databases\n",
    "* Hold large amounts of data\n",
    "* Support applications\n",
    "* Other databases are used for analyses\n",
    "\n",
    "#### Processing\n",
    "* Clean data\n",
    "* Aggregate data\n",
    "* Join data\n",
    "\n",
    "#### Scheduling\n",
    "* Plan jobs with specific intervals\n",
    "* Resolve dependency requirements of jobs\n",
    "\n",
    "#### Existing tools\n",
    "\n",
    "Databases:\n",
    "* MySQL\n",
    "* PostgreSQL\n",
    "\n",
    "Processing:\n",
    "* Spark\n",
    "* Hive\n",
    "\n",
    "Scheduling:\n",
    "* Apache Airflow\n",
    "* Oozie\n",
    "* Cron\n",
    "\n",
    "### Cloud providers\n",
    "\n",
    "#### Data processing in the cloud\n",
    "*Clusters of machines required*\n",
    "\n",
    "**Problem:** self-host data-center\n",
    "   * Cover electrical and maintenance costs\n",
    "   * Peak vs. quiet moments: hard to optimize\n",
    "   * **Solution:** use the cloud\n",
    "\n",
    "#### Data storage in the cloud\n",
    "*Reliability is required*\n",
    "\n",
    "**Problem:** self-host data-center\n",
    "* Disaster will strike\n",
    "* Need different geographical locations\n",
    "* **Solution:** use the cloud\n",
    "\n",
    "#### The big three: AWS, Azure and Google\n",
    "\n",
    "#### Storage\n",
    "*Upload files, e.g. storing product images*\n",
    "\n",
    "**Services**\n",
    "* AWS S3\n",
    "* Azure Blob Storage\n",
    "* Google Cloud Storage\n",
    "\n",
    "#### Computation\n",
    "*Performs calculations, e.g. hosting a web server*\n",
    "\n",
    "**Services**\n",
    "* AWS EC2\n",
    "* Azure Virtual Machines\n",
    "* Google Compute Engine\n",
    "\n",
    "#### Databases\n",
    "*Hold structured information*\n",
    "\n",
    "**Services**\n",
    "* AWS RDS\n",
    "* Azure SQL Database\n",
    "* Google Cloud SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2: Data Engineering Toolbox\n",
    "\n",
    "### Databases\n",
    "\n",
    "#### What are databases?\n",
    "> A usually large collection of data organized especially for rapid search and retrieval.\n",
    "\n",
    "* Holds data\n",
    "* Organizes data\n",
    "\n",
    "#### Databases and file storage\n",
    "* Databases: very organized, functionality like search, replication, etc.\n",
    "* File systems: less organized, simple, less functionality\n",
    "\n",
    "#### Structured vs. Unstructured data\n",
    "* Structured: database schema\n",
    "    * Relational database\n",
    "* Semi-structured\n",
    "    * JSON\n",
    "* Unstructured: schemaless, more like files\n",
    "    * Videos, photos\n",
    "    \n",
    "#### SQL and NoSQL\n",
    "SQL\n",
    "* Tables\n",
    "* Database schema\n",
    "* Relational databases\n",
    "* Example: MySQL, PostgreSQL\n",
    "\n",
    "No SQL\n",
    "* Non-relational databases\n",
    "* Structured or unstructured\n",
    "* Key-value stores (e.g. caching)\n",
    "* Document DB (e.g. JSON objects)\n",
    "* Example: redis, mongoDB\n",
    "\n",
    "#### SQL: The database schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL\n",
    "\n",
    "\"\"\"\n",
    "-- Create Customer Table\n",
    "CREATE TABLE \"Customer\" (\n",
    "    \"id\" SERIAL NOT NULL,\n",
    "    \"first_name\" varchar,\n",
    "    \"last_name\" varchar,\n",
    "    PRIMARY KEY (\"id\")\n",
    ");\n",
    "\n",
    "-- Create Order Table (\n",
    "    \"id\" SERIAL NTO NULL,\n",
    "    \"customer_id\" integer REGERENCES \"Customer\",\n",
    "    \"product_name\" varchar,\n",
    "    \"product_price\" integer,\n",
    "    PRIMARY KEY (\"id\")\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SQL: Star schema\n",
    "> *The star schema consists of one or more fact tables referencing any number of dimention tables.*\n",
    "\n",
    "* **Facts:** Things that happened (e.g. Product Orders)\n",
    "* **Dimensions:** information on the world (e.g. Customer Information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the SELECT statement\n",
    "data = pd.read_sql(\"\"\"\n",
    "SELECT first_name, last_name FROM \"Customer\"\n",
    "ORDER BY last_name, first_name \n",
    "\"\"\", db_engine)\n",
    "\n",
    "# Show the first 3 rows of the DataFrame\n",
    "print(data.head(3))\n",
    "\n",
    "# Show the info of the DataFrame\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the SELECT statement\n",
    "data = pd.read_sql(\"\"\"\n",
    "SELECT * FROM \"Customer\"\n",
    "INNER JOIN \"Order\"\n",
    "ON \"Order\".\"customer_id\"=\"Customer\".\"id\"\n",
    "\"\"\", db_engine)\n",
    "\n",
    "# Show the id column of data\n",
    "print(data.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is parallel computing\n",
    "\n",
    "#### Idea behind parallel computing\n",
    "*Basis of modern data processing tools*\n",
    "\n",
    "* Memory\n",
    "* Processing power\n",
    "\n",
    "Idea:\n",
    "* Split task into subtasks\n",
    "* Distribute subtasks over several computers\n",
    "* Work together to finish task\n",
    "\n",
    "#### Benefits of parallel computing\n",
    "* Processing power\n",
    "* Memory: partition the dataset\n",
    "\n",
    "#### Risks of parallel computing\n",
    "*Overhead due to communication*\n",
    "\n",
    "* Task needs to be large\n",
    "* Need several processing units\n",
    "\n",
    "`multiprocessing.Pool`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "def take_mean_age(year_and_group):\n",
    "    year, group = year_and_group\n",
    "    return pd.DataFrame({\"Age\": group[\"Age\"].mean()}, index=[year])\n",
    "\n",
    "with Pool(4): as p:\n",
    "    results = p.map(take_mean_age, athlete_event.groupby(\"Year\"))\n",
    "    \n",
    "result_df = pd.concat(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`dask`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# Partition dataframe into 4\n",
    "athlete_events_dask = dd.from_pandas(athlete_events, npartitions = 4)\n",
    "\n",
    "# Run parallel computations on each partition\n",
    "result_df = athlete_events_dask.groupby('Year').Age.mean().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply a function over multiple cores\n",
    "@print_timing\n",
    "def parallel_apply(apply_func, groups, nb_cores):\n",
    "    with Pool(nb_cores) as p:\n",
    "        results = p.map(apply_func, groups)\n",
    "    return pd.concat(results)\n",
    "\n",
    "# Parallel apply using 1 core\n",
    "parallel_apply(take_mean_age, athlete_events.groupby('Year'), 1)\n",
    "\n",
    "# Parallel apply using 2 cores\n",
    "parallel_apply(take_mean_age, athlete_events.groupby('Year'), 2)\n",
    "\n",
    "# Parallel apply using 4 cores\n",
    "parallel_apply(take_mean_age, athlete_events.groupby('Year'), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# Set the number of pratitions\n",
    "athlete_events_dask = dd.from_pandas(athlete_events, npartitions = 4)\n",
    "\n",
    "# Calculate the mean Age per Year\n",
    "print(athlete_events_dask.groupby('Year').Age.mean().compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel Computation Frameworks\n",
    "\n",
    "#### Apache Hadoop\n",
    "\n",
    "#### HDFS\n",
    "* Distributed File System\n",
    "* Similar to the file system on your computer, the only difference being that the files reside on multiple different computers\n",
    "\n",
    "#### MapReduce\n",
    "* Works similarly to what we've seen before\n",
    "* Has flaws; one of which was that it was hard to write these MapReduce jobs\n",
    "* Hive popped up to address this problem\n",
    "\n",
    "#### Hive\n",
    "* Runs on Hadoop\n",
    "* Structured Query Language: Hive SQL\n",
    "* Initially MapReduce, now other tools\n",
    "\n",
    "#### Hive: an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indistinguishable from a regular SQL query\n",
    "SELECT year, AVG(age)\n",
    "FROM views.athlete_events\n",
    "GROUP BY year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apache Spark\n",
    "* Avoid disk writes\n",
    "* Maintained by Apache Software Foundation\n",
    "\n",
    "#### Resilient distributed datasets (RDD)\n",
    "* Spark relies on them\n",
    "* Similar to list of tuples\n",
    "* Transformations: `.map()` or `.filter()`\n",
    "* Actions: `.count()` or `.first()`\n",
    "\n",
    "#### PySpark\n",
    "* Python interface to Spark\n",
    "* DataFrame abstraction\n",
    "* Looks similar to Pandas\n",
    "\n",
    "#### PySpark: an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset into athlete_events_spark first\n",
    "\n",
    "(athlete_events_spark\n",
    "    .groupBy('Year')\n",
    "    .mean('Age')\n",
    "    .show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySpark groupby example\n",
    "\n",
    "# Print the type of athlete_events_spark\n",
    "print(type(athlete_events_spark))\n",
    "\n",
    "# Print the schema of athlete_events_spark\n",
    "print(athlete_events_spark.printSchema())\n",
    "\n",
    "# Group by the Year, and find the mean Age\n",
    "print(athlete_events_spark.groupBy('Year').mean('Age'))\n",
    "\n",
    "# Group by the Year, and find the mean Age\n",
    "print(athlete_events_spark.groupBy('Year').mean('Age').show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow scheduling frameworks\n",
    "\n",
    "#### An example pipeline\n",
    "* CSV --> Spark --> load into SQL database\n",
    "* *How to schedule?*\n",
    "    * Manually\n",
    "    * `cron` scheduling tool\n",
    "    * What about dependencies?\n",
    "\n",
    "#### DAGs\n",
    "*Directed Acyclic Graph*\n",
    "* Set of nodes\n",
    "* Directed edges\n",
    "* No cycles\n",
    "\n",
    "#### The tools for the job\n",
    "* Linux's `cron`\n",
    "* Spotify's Luigi\n",
    "* Apache Airflow\n",
    "\n",
    "#### Apache Airflow\n",
    "* Created at Airbnb\n",
    "* DAGs\n",
    "* Python\n",
    "\n",
    "#### Airflow: an example in code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DAG object\n",
    "dag = DAG(dag_id=\"example_dag\", ... , schedule_interval=\"0 * * * *\")\n",
    "\n",
    "# Define operations\n",
    "start_cluster = StartClusterOperator(task_id='start_cluster', dag=dag)\n",
    "ingest_customer_data = SparkJobOperator(task_id='ingest_customer_data', dag=dag)\n",
    "ingest_product_data = SparkJobOperator(taks_id='ingest_product_data', dag=dag)\n",
    "enrich_customer_data = PythonOperator(task_id='enrich_customer_data', ..., dag=dag)\n",
    "\n",
    "# Set up dependency flow\n",
    "start_cluster.set_downstream(ingest_customer_data)\n",
    "ingest_cusomter_data.set_downstream(enrich_customer_data)\n",
    "ingest_product_data.set_downstream(enrich_customer_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 3: Extract, Transform and Load (ETL)\n",
    "\n",
    "### Extract\n",
    "\n",
    "#### Extracting data: what does it mean?\n",
    "* Extracting the data from persistent storage, such as an Amazon file from S3 or a sql database\n",
    "\n",
    "#### Extract form text files\n",
    "\n",
    "Unstructured\n",
    "* Plain test\n",
    "* E.g. chapter from a book\n",
    "\n",
    "Flat files\n",
    "* Row = record\n",
    "* Column = attribute\n",
    "* E.g. `.tsv` or `.csv`\n",
    "\n",
    "#### JSON\n",
    "* JavaScript Object Notation\n",
    "* Semi-structured\n",
    "* Atomic\n",
    "    * `number`\n",
    "    * `string`\n",
    "    * `boolean`\n",
    "    * `null`\n",
    "* Composite\n",
    "    * `array`\n",
    "    * `object`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value_1\n"
     ]
    }
   ],
   "source": [
    "# json package helps to parse json data into python dictionary\n",
    "import json\n",
    "result = json.loads('{\"key_1\": \"value_1\", \"key_2\": \"value_2\"}')\n",
    "print(result[\"key_1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data on the Web\n",
    "\n",
    "Requests\n",
    "* request page <--> response with data\n",
    "\n",
    "Example\n",
    "1. Browse to Google\n",
    "2. Request to Google Server\n",
    "3. Google responds\n",
    "\n",
    "#### Data on the Web through APIs\n",
    "* Send data in JSON format\n",
    "* API: application programming interface\n",
    "* Examples\n",
    "    * Twitter API\n",
    "    * Hackernews API\n",
    "    \n",
    "#### Data in databases\n",
    "\n",
    "Application databases\n",
    "* Tranactions\n",
    "* Inserts or changes\n",
    "* OLTP (Online transaction processing)\n",
    "* Row-oriented\n",
    "\n",
    "Analytical databases\n",
    "* OLAP (Online analytical processing)\n",
    "* Column-oriented\n",
    "\n",
    "#### Extraction from databases\n",
    "\n",
    "Connection string/URI:\n",
    "\n",
    "`postgresql://[user[:password]@[host][:post]`\n",
    "\n",
    "Use in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy\n",
    "connection_url = \"postgresql://repl:password@localhost:5432/pagila\"\n",
    "db_engine = sqlalchemy.create_engine(connection_uri)\n",
    "\n",
    "import pandas as pd\n",
    "pd.read_sql(\"SELECT * FROM customer\", db_engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fetch from an API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'by': 'neis', 'descendants': 0, 'id': 16222426, 'score': 17, 'time': 1516800333, 'title': 'Duolingo-Style Learning for Data Science: DataCamp for Mobile', 'type': 'story', 'url': 'https://medium.com/datacamp/duolingo-style-learning-for-data-science-datacamp-for-mobile-3861d1bc02df'}\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Fetch the Hackernews post\n",
    "resp = requests.get(\"https://hacker-news.firebaseio.com/v0/item/16222426.json\")\n",
    "\n",
    "# Print the response parsed as JSON\n",
    "print(resp.json())\n",
    "\n",
    "# Assign the score of the test to post_score\n",
    "post_score = resp.json()[\"score\"]\n",
    "print(post_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read from a database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy\n",
    "\n",
    "# Function to extract table to a pandas DataFrame\n",
    "def extract_table_to_pandas(tablename, db_engine):\n",
    "    query = \"SELECT * FROM {}\".format(tablename)\n",
    "    return pd.read_sql(query, db_engine)\n",
    "\n",
    "# Connect to the database using the connection URI\n",
    "connection_uri = \"postgresql://repl:password@localhost:5432/pagila\" \n",
    "db_engine = sqlalchemy.create_engine(connection_uri)\n",
    "\n",
    "# Extract the film table into a pandas DataFrame\n",
    "extract_table_to_pandas(\"film\", db_engine)\n",
    "\n",
    "# Extract the customer table into a pandas DataFrame\n",
    "extract_table_to_pandas(\"customer\", db_engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform\n",
    "\n",
    "#### Kind of transformations\n",
    "* Selection of attribute (e.g. 'email')\n",
    "* Translation of code values (e.g. 'New York' -> 'NY')\n",
    "* Data Validation (e.g. date input in 'created_at')\n",
    "* Splitting columns into multiple columns\n",
    "* Joining from multiple sources\n",
    "\n",
    "#### An example: split (Pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>email</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>jane.doe@theweb.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_id                email\n",
       "0            1  jane.doe@theweb.com"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Pandas DataFrame with customer data\n",
    "customer_df = pd.DataFrame({'customer_id': [1]\n",
    "                           , 'email':['jane.doe@theweb.com']})\n",
    "customer_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>email</th>\n",
       "      <th>username</th>\n",
       "      <th>domain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>jane.doe@theweb.com</td>\n",
       "      <td>jane.doe</td>\n",
       "      <td>theweb.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_id                email  username      domain\n",
       "0            1  jane.doe@theweb.com  jane.doe  theweb.com"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split email column into two columns\n",
    "split_email = customer_df.email.str.split(\"@\", expand=True)\n",
    "\n",
    "# At this point, split_email will have 2 columns, a first\n",
    "# one with everything before @, and a second one with\n",
    "# everything after @\n",
    "\n",
    "# Create 2 new columns using the resulting DataFrame.\n",
    "customer_df = customer_df.assign(\n",
    "    username=split_email[0],\n",
    "    domain=split_email[1]\n",
    ")\n",
    "\n",
    "customer_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transforming in PySpark\n",
    "\n",
    "Extract data into PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql\n",
    "\n",
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()\n",
    "\n",
    "spark.read.jdbc(\"jdbc:postgresql://localhost:5432/pagila\",\n",
    "                \"customer\",\n",
    "                properties = {\"user\":\"repl\", \"password\":\"password\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An example: join (PySpark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df # PySpark DataFrame with customer data\n",
    "ratings_df # PySpark DataFrame with ratings data\n",
    "\n",
    "# Groupby ratings\n",
    "ratings_per_customer = ratings_df.groupBy(\"customer_id\").mean(\"rating\")\n",
    "\n",
    "# Join on customer ID\n",
    "customer_df.join(\n",
    "    ratings_per_cusotmer,\n",
    "    customer_df.customer_id==ratings_per_customer.customer_id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading\n",
    "\n",
    "#### Analytics or applications databases\n",
    "Analytics\n",
    "* Aggregate queries\n",
    "* Online ananlytical processing (OLAP)\n",
    "\n",
    "Applications\n",
    "* Lots of transactions\n",
    "* Online transaction processing (OLTP)\n",
    "\n",
    "#### Column- and row-oriented\n",
    "Analytics\n",
    "* Column-oriented\n",
    "* Queries about subset of columns\n",
    "* Parallelization\n",
    "\n",
    "Applications\n",
    "* Row-oriented\n",
    "* Stored per record\n",
    "* Added per transaction\n",
    "* E.g. adding a customer is fast\n",
    "\n",
    "#### MPP Databases\n",
    "*Massively Parallel Processing Databases*\n",
    "* Amazon Redshift\n",
    "* Azure SQL Data Warehouse\n",
    "* Google BigQuery\n",
    "\n",
    "#### An example: Redshift\n",
    "*Load from file to columnar storage format*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas .to_parquet() method\n",
    "df.to_parquet(\"./s3://path/to/bucket/customer.parquet\")\n",
    "\n",
    "# PySpark .write.parquet() method\n",
    "df.write.parquet(\"/.s3://path/to/bucket/customer.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COPY customer\n",
    "FROM 's3://path/to/bucket/customer.parquet'\n",
    "FORMAT as parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load to PostgreSQL\n",
    "`pandas.to_sql()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation on data\n",
    "recommendations = transform_find_recommendations(ratings_df)\n",
    "\n",
    "# Load into PostgreSQL database\n",
    "recommendations.to_sql(\"recommendations\",\n",
    "                       db_engine,\n",
    "                       schema=\"store\",\n",
    "                       if_exists=\"replace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finish the connection URI\n",
    "connection_uri = \"postgresql://repl:password@localhost:5432/dwh\"\n",
    "db_engine_dwh = sqlalchemy.create_engine(connection_uri)\n",
    "\n",
    "# Transformation step, join with recommendations data\n",
    "film_pdf_joined = film_pdf.join(recommendations)\n",
    "\n",
    "# Finish the .to_sql() call to write to store.film\n",
    "film_pdf_joined.to_sql(\"film\", db_engine_dwh, schema=\"store\", if_exists=\"replace\")\n",
    "\n",
    "# Run the query to fetch the data\n",
    "pd.read_sql(\"SELECT film_id, recommended_film_ids FROM store.film\", db_engine_dwh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together\n",
    "\n",
    "#### The ETL function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_table_to_df(tablename, db_engine):\n",
    "    return pd.read_sql(\"SELECT * FROM {}\".format(tablename), db_engine)\n",
    "\n",
    "def split_columns_transform(df, column, pat, suffixes):\n",
    "    # Converts column into str and splits it on pat...\n",
    "\n",
    "def load_df_into_dwh(film_df, tablename, schema, db_engine):\n",
    "    return pd.to_sql(tablename, db_engine, schema=schema, if_exists=\"replace\")\n",
    "\n",
    "db_engines = { ... } # Needs to be configured\n",
    "\n",
    "def etl():\n",
    "    \n",
    "    # Extract\n",
    "    film_df = extract_table_to_df(\"film\", db_engines[\"store\"])\n",
    "    \n",
    "    # Transform\n",
    "    film_df = split_columns_transform(film_df, \"rental_rate\", '.', [\"_dollar\", \"_cents\"])\n",
    "    \n",
    "    # Load\n",
    "    load_df_into_dwh(film_df, \"film\", \"store\", db_engine[\"dwh\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Airflow refresher\n",
    "* Workflow scheduler\n",
    "* Python\n",
    "* DAGs\n",
    "* Tasks defined in operators (e.g. `BashOperator`)\n",
    "\n",
    "#### Scheduling with DAGs in Airflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.models import DAG\n",
    "\n",
    "dag = DAG(dag_id=\"sample\",\n",
    "          ..., \n",
    "          schedule_interval=\"0 0 * * *\")\n",
    "\n",
    "\"\"\"\n",
    "cron expression:\n",
    "    * minute (0 - 59)\n",
    "    * hour (0 - 23)\n",
    "    * day of the month (1 - 31)\n",
    "    * month (1 - 12)\n",
    "    * day of the week (0 - 6) \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To learn more about cron jobs, go to https://crontab.guru\n",
    "\n",
    "#### The DAG definition file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.models import DAG\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "\n",
    "dag = DAG(dag_id=\"etl_pipeline\",\n",
    "          schedule_interval=\"0 0 * * *\")\n",
    "\n",
    "etl_task = PythonOperator(task_id=\"etl_task\",\n",
    "                          python_callable=etl,\n",
    "                          dag=dag)\n",
    "\n",
    "etl_task.set_upstream(wait_for_this_task)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_python_toolkit",
   "language": "python",
   "name": "venv_python_toolkit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
